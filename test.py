import os
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import base64
from io import BytesIO

os.environ["SD_ENABLE_ASIO"] = "1"
import sounddevice as sd

# --- ì„¤ì • ---
ASIO_DEVICE_ID = 18
SAMPLE_RATE = 44100
RECORD_DURATION = 10  # 10ì´ˆ ë…¹ìŒ
COUNTIN_DURATION = 3  # 3ì´ˆ ì¹´ìš´íŠ¸ì¸
SOFTWARE_GAIN = 60.0
METRONOME_BPM = 120  # ë©”íŠ¸ë¡œë†ˆ BPM
BLOCK_SIZE = 64  # ì˜¤ë””ì˜¤ ë¸”ë¡ í¬ê¸°
CHROMATIC_ENABLED = True  # ì‹œê°í™” ê·¸ë¦¬ë“œì—ëŠ” 8ë¶„ìŒí‘œ í‘œì‹œ (ì†Œë¦¬ëŠ” ì•ˆ ë‚¨)

# ë…¹ìŒ ë²„í¼
recorded_data = []
is_recording = False

# ë©”íŠ¸ë¡œë†ˆ ì‚¬ìš´ë“œ ìƒì„±
def generate_metronome_click(duration_ms=50, frequency=1000, sample_rate=44100):
    samples = int(sample_rate * duration_ms / 1000)
    t = np.linspace(0, duration_ms/1000, samples, False)
    tone = np.sin(2 * np.pi * frequency * t)
    envelope = np.exp(-10 * t)
    return (tone * envelope * 0.3).astype(np.float32)

def generate_countin_click(duration_ms=80, frequency=1200, sample_rate=44100):
    samples = int(sample_rate * duration_ms / 1000)
    t = np.linspace(0, duration_ms/1000, samples, False)
    tone = np.sin(2 * np.pi * frequency * t)
    envelope = np.exp(-8 * t)
    return (tone * envelope * 0.5).astype(np.float32)

METRONOME_SOUND = generate_metronome_click()
# ì²« ë°•ìë¥¼ êµ¬ë¶„í•˜ê³  ì‹¶ì„ ê²½ìš° ì‚¬ìš©í•  ì‚¬ìš´ë“œ
DOWNBEAT_SOUND = generate_metronome_click(frequency=1200) 

metronome_active = False
current_beat = 0
beat_interval_samples = 0
sample_counter = 0

def audio_callback(indata, outdata, frames, time_info, status):
    global recorded_data, metronome_active, current_beat, sample_counter, is_recording
    
    guitar_input = indata[:, 0]
    amplified = np.clip(guitar_input * SOFTWARE_GAIN, -1.0, 1.0)
    
    if is_recording:
        recorded_data.extend(amplified)
    
    output_signal = amplified.copy()
    
    if metronome_active:
        for i in range(frames):
            # ë°•ì ê°„ê²©(ìƒ˜í”Œ ìˆ˜)ì´ ì§€ë‚˜ë©´ ì¹´ìš´í„° ë¦¬ì…‹ ë° ë¹„íŠ¸ ì¦ê°€
            if sample_counter >= beat_interval_samples:
                sample_counter = 0
                current_beat = (current_beat + 1) % 4
            
            # ëª¨ë“  4ë°•ì(0, 1, 2, 3)ì—ì„œ ì†Œë¦¬ ë°œìƒ
            # ì—¬ê¸°ì„œëŠ” 8ë¶„ìŒí‘œ ì†Œë¦¬(CHROMATIC_SOUND) ë¡œì§ì„ ì œì™¸í•˜ì—¬ 4ë¹„íŠ¸ë¡œë§Œ ë“¤ë¦¬ê²Œ í•¨
            if sample_counter < len(METRONOME_SOUND):
                # ì²« ë°•ì(ê°•ë°•)ë§Œ ì†Œë¦¬ë¥¼ ì•½ê°„ ë‹¤ë¥´ê²Œ í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ ê°€ëŠ¥
                # if current_beat == 0:
                #     output_signal[i] += DOWNBEAT_SOUND[sample_counter]
                # else:
                #     output_signal[i] += METRONOME_SOUND[sample_counter]
                output_signal[i] += METRONOME_SOUND[sample_counter]
            
            sample_counter += 1
    
    output_signal = np.clip(output_signal, -1.0, 1.0)
    for i in range(outdata.shape[1]):
        outdata[:, i] = output_signal

def create_waveform_with_metronome(audio_data, bpm, sample_rate):
    """íŒŒí˜•ê³¼ ì‹¬í”Œ ê·¸ë¦¬ë“œ ì‹œê°í™”"""
    duration = len(audio_data) / sample_rate
    time_axis = np.linspace(0, duration, len(audio_data))
    
    fig, ax = plt.subplots(figsize=(18, 7), dpi=100)
    
    # íŒŒí˜• ê·¸ë¦¬ê¸°
    ax.plot(time_axis, audio_data, color='#2E86DE', linewidth=0.5, alpha=0.8)
    ax.fill_between(time_axis, audio_data, alpha=0.3, color='#2E86DE')
    
    # ë©”íŠ¸ë¡œë†ˆ ê·¸ë¦¬ë“œ ê³„ì‚°
    beat_interval = 60.0 / bpm
    
    # 4ë°•ì ë‹¨ìœ„ ê·¸ë¦¬ë“œ (ë¹¨ê°„ ì‹¤ì„ /ì ì„ )
    beat_positions = np.arange(0, duration, beat_interval)
    for i, pos in enumerate(beat_positions):
        if i % 4 == 0:
            ax.axvline(pos, color='#FF4757', linestyle='-', linewidth=1.5, alpha=0.8)
        else:
            ax.axvline(pos, color='#FF6B6B', linestyle='--', linewidth=1.0, alpha=0.6)
    
    # ì‹œê°ì  ë¶„ì„ì„ ìœ„í•œ 8ë¶„ìŒí‘œ ë³´ì¡°ì„  (ì†Œë¦¬ëŠ” ë‚˜ì§€ ì•Šì§€ë§Œ ì—°ì£¼ íƒ€ì´ë° í™•ì¸ìš©)
    if CHROMATIC_ENABLED:
        chromatic_positions = np.arange(beat_interval / 2, duration, beat_interval)
        for pos in chromatic_positions:
            ax.axvline(pos, color='#70a1ff', linestyle=':', linewidth=0.8, alpha=0.3)
    
    # ìŠ¤íƒ€ì¼ë§
    ax.set_xlim(0, duration)
    ax.set_ylim(-1.1, 1.1)
    ax.set_xlabel('Time (seconds)', fontsize=13, fontweight='bold')
    ax.set_ylabel('Amplitude', fontsize=13, fontweight='bold')
    
    title = f'Guitar Performance Analysis | {bpm} BPM | 4-Beat Metronome'
    ax.set_title(title, fontsize=15, fontweight='bold', pad=20)
    
    ax.grid(True, alpha=0.2, linestyle='-', linewidth=0.5)
    ax.set_facecolor('#F8F9FA')
    
    # ë²”ë¡€
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='#2E86DE', alpha=0.3, label='Guitar Signal'),
        plt.Line2D([0], [0], color='#FF4757', linestyle='-', linewidth=1.5, label='Downbeat (Beat 1)'),
        plt.Line2D([0], [0], color='#FF6B6B', linestyle='--', linewidth=1.0, label='Beats 2, 3, 4'),
    ]
    ax.legend(handles=legend_elements, loc='upper right', fontsize=10)
    
    plt.tight_layout()
    return fig

def fig_to_base64(fig):
    buf = BytesIO()
    fig.savefig(buf, format='png', dpi=150, bbox_inches='tight')
    buf.seek(0)
    img_base64 = base64.b64encode(buf.read()).decode('utf-8')
    buf.close()
    return img_base64

def save_analysis_image(fig, filename=None):
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"guitar_chromatic_{timestamp}.png"
    
    fig.savefig(filename, dpi=150, bbox_inches='tight')
    print(f"âœ“ ì´ë¯¸ì§€ ì €ì¥: {filename}")
    return filename

def record_and_analyze():
    global recorded_data, metronome_active, current_beat, sample_counter, beat_interval_samples, is_recording
    recorded_data = []
    current_beat = 0
    sample_counter = 0
    is_recording = False
    
    # 1ë°•ì— í•´ë‹¹í•˜ëŠ” ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
    beat_interval_samples = int(SAMPLE_RATE * 60.0 / METRONOME_BPM)
    
    print(f"{'='*70}")
    print(f"ğŸ¸ 4ë¹„íŠ¸ ë©”íŠ¸ë¡œë†ˆ í¬ë¡œë§¤í‹± ë¶„ì„ê¸° ({METRONOME_BPM} BPM)")
    print(f"{'='*70}")
    print(f"â±ï¸  ì¹´ìš´íŠ¸ì¸: {COUNTIN_DURATION}ì´ˆ")
    print(f"ğŸ“Š ë…¹ìŒ ì‹œê°„: {RECORD_DURATION}ì´ˆ")
    print(f"ğŸµ ì„¤ì •: ëª¨ë“  4ë°•ì ì†Œë¦¬ ì¶œë ¥ (8ë¶„ìŒí‘œ ì†Œë¦¬ ì œê±°)")
    print(f"{'='*70}\n")
    
    try:
        info = sd.query_devices(ASIO_DEVICE_ID)
        channels = min(info['max_input_channels'], info['max_output_channels'])
        
        with sd.Stream(device=ASIO_DEVICE_ID,
                       samplerate=SAMPLE_RATE,
                       blocksize=BLOCK_SIZE,
                       dtype='float32',
                       channels=channels,
                       callback=audio_callback):
            
            metronome_active = True
            
            print("ğŸ¼ ì¹´ìš´íŠ¸ì¸ ì‹œì‘! 4ë¹„íŠ¸ ì†Œë¦¬ì— ë§ì¶”ì–´ ì¤€ë¹„í•˜ì„¸ìš”...")
            sd.sleep(COUNTIN_DURATION * 1000)
            
            print("\nğŸš€ ë…¹ìŒ ì‹œì‘! í¬ë¡œë§¤í‹± ì—°ì£¼ Go!\n")
            is_recording = True
            
            for i in range(RECORD_DURATION, 0, -1):
                bar = 'â–ˆ' * (RECORD_DURATION - i + 1) + 'â–‘' * (i - 1)
                print(f"  â±ï¸  [{bar}] {i:2d}ì´ˆ ë‚¨ìŒ", end='\r')
                sd.sleep(1000)
            
            metronome_active = False
            is_recording = False
        
        print("\n\nâœ“ ë…¹ìŒ ì™„ë£Œ! ë¶„ì„ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
        
        audio_array = np.array(recorded_data)
        fig = create_waveform_with_metronome(audio_array, METRONOME_BPM, SAMPLE_RATE)
        
        filename = save_analysis_image(fig)
        img_base64 = fig_to_base64(fig)
        
        print("\n" + "="*70)
        print("ğŸ“Š ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ!")
        print("="*70)
        print(f"âœ… ì´ë¯¸ì§€ íŒŒì¼: {filename}")
        print("="*70)
        
        plt.show()
        
        return filename, img_base64
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    record_and_analyze()