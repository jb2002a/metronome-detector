import os
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import base64
from io import BytesIO

os.environ["SD_ENABLE_ASIO"] = "1"
import sounddevice as sd

# --- ì„¤ì • ---
ASIO_DEVICE_ID = 18
SAMPLE_RATE = 44100
RECORD_DURATION = 10          # 10ì´ˆ ë…¹ìŒ

# ì¹´ìš´íŠ¸ì¸(ì´ˆ) -> ë§ˆë”” ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½
COUNTIN_BARS = 1              # ì¹´ìš´íŠ¸ì¸ ë§ˆë”” ìˆ˜ (4/4 ê¸°ì¤€)
BEATS_PER_BAR = 4             # 4/4

SOFTWARE_GAIN = 60.0
METRONOME_BPM = 120           # ë©”íŠ¸ë¡œë†ˆ BPM
BLOCK_SIZE = 64               # ì˜¤ë””ì˜¤ ë¸”ë¡ í¬ê¸°
CHROMATIC_ENABLED = True      # ì‹œê°í™” ê·¸ë¦¬ë“œì—ëŠ” 8ë¶„ìŒí‘œ í‘œì‹œ (ì†Œë¦¬ëŠ” ì•ˆ ë‚¨)

# ë…¹ìŒ ë²„í¼
recorded_data = []
is_recording = False

# --- ìœ í‹¸: ë§ˆë”” ê¸°ë°˜ ì¹´ìš´íŠ¸ì¸ ì‹œê°„ì„ ìƒ˜í”Œ ì •ë ¬ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚° ---
def bars_to_sleep_ms(countin_bars, bpm, sample_rate, beats_per_bar=4):
    """
    countin_bars(ë§ˆë””) -> sleepì— ì“¸ ë°€ë¦¬ì´ˆ(ms)ë¡œ ë³€í™˜.
    ìƒ˜í”Œ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°í•´ ë©”íŠ¸ë¡œë†ˆ ê·¸ë¦¬ë“œ/ë…¹ìŒ ì‹œì‘ ì •ë ¬ì„ ë” ì •í™•íˆ ë§ì¶¤.
    """
    samples_per_beat = int(sample_rate * 60.0 / bpm)
    total_beats = int(countin_bars * beats_per_bar)
    total_samples = total_beats * samples_per_beat
    total_ms = int(total_samples / sample_rate * 1000)
    return total_ms, total_beats

# ë©”íŠ¸ë¡œë†ˆ ì‚¬ìš´ë“œ ìƒì„±
def generate_metronome_click(duration_ms=50, frequency=1000, sample_rate=44100):
    samples = int(sample_rate * duration_ms / 1000)
    t = np.linspace(0, duration_ms / 1000, samples, False)
    tone = np.sin(2 * np.pi * frequency * t)
    envelope = np.exp(-10 * t)
    return (tone * envelope * 0.3).astype(np.float32)

def generate_countin_click(duration_ms=80, frequency=1200, sample_rate=44100):
    # í˜„ì¬ êµ¬í˜„ì—ì„œëŠ” ë³„ë„ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, í•„ìš” ì‹œ ì¹´ìš´íŠ¸ì¸ í´ë¦­ìŒì„ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ìœ ì§€
    samples = int(sample_rate * duration_ms / 1000)
    t = np.linspace(0, duration_ms / 1000, samples, False)
    tone = np.sin(2 * np.pi * frequency * t)
    envelope = np.exp(-8 * t)
    return (tone * envelope * 0.5).astype(np.float32)

METRONOME_SOUND = generate_metronome_click()
# ì²« ë°•ìë¥¼ êµ¬ë¶„í•˜ê³  ì‹¶ì„ ê²½ìš° ì‚¬ìš©í•  ì‚¬ìš´ë“œ(í˜„ì¬ëŠ” ì£¼ì„ ì²˜ë¦¬ëœ ë¡œì§ì´ ì¡´ì¬)
DOWNBEAT_SOUND = generate_metronome_click(frequency=1200)

metronome_active = False
current_beat = 0
beat_interval_samples = 0
sample_counter = 0

def audio_callback(indata, outdata, frames, time_info, status):
    global recorded_data, metronome_active, current_beat, sample_counter, is_recording

    guitar_input = indata[:, 0]
    amplified = np.clip(guitar_input * SOFTWARE_GAIN, -1.0, 1.0)

    if is_recording:
        recorded_data.extend(amplified)

    output_signal = amplified.copy()

    if metronome_active:
        for i in range(frames):
            # ë°•ì ê°„ê²©(ìƒ˜í”Œ ìˆ˜)ì´ ì§€ë‚˜ë©´ ì¹´ìš´í„° ë¦¬ì…‹ ë° ë¹„íŠ¸ ì¦ê°€
            if sample_counter >= beat_interval_samples:
                sample_counter = 0
                current_beat = (current_beat + 1) % 4

            # ëª¨ë“  4ë°•ìì—ì„œ ì†Œë¦¬ ë°œìƒ
            if sample_counter < len(METRONOME_SOUND):

                if current_beat == 0:
                    output_signal[i] += DOWNBEAT_SOUND[sample_counter]
                else:
                    output_signal[i] += METRONOME_SOUND[sample_counter]


            sample_counter += 1

    output_signal = np.clip(output_signal, -1.0, 1.0)
    for ch in range(outdata.shape[1]):
        outdata[:, ch] = output_signal

def create_waveform_with_metronome(audio_data, bpm, sample_rate):
    """íŒŒí˜•ê³¼ ì‹¬í”Œ ê·¸ë¦¬ë“œ ì‹œê°í™”"""
    duration = len(audio_data) / sample_rate
    time_axis = np.linspace(0, duration, len(audio_data))

    fig, ax = plt.subplots(figsize=(18, 7), dpi=100)

    # íŒŒí˜• ê·¸ë¦¬ê¸°
    ax.plot(time_axis, audio_data, color="#2E86DE", linewidth=0.5, alpha=0.8)
    ax.fill_between(time_axis, audio_data, alpha=0.3, color="#2E86DE")

    # ë©”íŠ¸ë¡œë†ˆ ê·¸ë¦¬ë“œ ê³„ì‚°
    beat_interval = 60.0 / bpm

    # 4ë°•ì ë‹¨ìœ„ ê·¸ë¦¬ë“œ (ë¹¨ê°„ ì‹¤ì„ /ì ì„ )
    beat_positions = np.arange(0, duration, beat_interval)
    for i, pos in enumerate(beat_positions):
        if i % 4 == 0:
            ax.axvline(pos, color="#FF4757", linestyle="-", linewidth=1.5, alpha=0.8)
        else:
            ax.axvline(pos, color="#FF6B6B", linestyle="--", linewidth=1.0, alpha=0.6)

    # ì‹œê°ì  ë¶„ì„ì„ ìœ„í•œ 8ë¶„ìŒí‘œ ë³´ì¡°ì„  (ì†Œë¦¬ëŠ” ë‚˜ì§€ ì•Šì§€ë§Œ ì—°ì£¼ íƒ€ì´ë° í™•ì¸ìš©)
    if CHROMATIC_ENABLED:
        chromatic_positions = np.arange(beat_interval / 2, duration, beat_interval)
        for pos in chromatic_positions:
            ax.axvline(pos, color="#70a1ff", linestyle=":", linewidth=0.8, alpha=0.3)

    # ìŠ¤íƒ€ì¼ë§
    ax.set_xlim(0, duration)
    ax.set_ylim(-1.1, 1.1)
    ax.set_xlabel("Time (seconds)", fontsize=13, fontweight="bold")
    ax.set_ylabel("Amplitude", fontsize=13, fontweight="bold")

    title = f"Guitar Performance Analysis | {bpm} BPM | 4-Beat Metronome"
    ax.set_title(title, fontsize=15, fontweight="bold", pad=20)

    ax.grid(True, alpha=0.2, linestyle="-", linewidth=0.5)
    ax.set_facecolor("#F8F9FA")

    # ë²”ë¡€
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor="#2E86DE", alpha=0.3, label="Guitar Signal"),
        plt.Line2D([0], [0], color="#FF4757", linestyle="-", linewidth=1.5, label="Downbeat (Beat 1)"),
        plt.Line2D([0], [0], color="#FF6B6B", linestyle="--", linewidth=1.0, label="Beats 2, 3, 4"),
    ]
    ax.legend(handles=legend_elements, loc="upper right", fontsize=10)

    plt.tight_layout()
    return fig

def fig_to_base64(fig):
    buf = BytesIO()
    fig.savefig(buf, format="png", dpi=150, bbox_inches="tight")
    buf.seek(0)
    img_base64 = base64.b64encode(buf.read()).decode("utf-8")
    buf.close()
    return img_base64

def save_analysis_image(fig, filename=None):
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"guitar_chromatic_{timestamp}.png"

    fig.savefig(filename, dpi=150, bbox_inches="tight")
    print(f"âœ“ ì´ë¯¸ì§€ ì €ì¥: {filename}")
    return filename

def record_and_analyze():
    global recorded_data, metronome_active, current_beat, sample_counter, beat_interval_samples, is_recording
    recorded_data = []
    current_beat = 0
    sample_counter = 0
    is_recording = False

    # 1ë°•ì— í•´ë‹¹í•˜ëŠ” ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
    beat_interval_samples = int(SAMPLE_RATE * 60.0 / METRONOME_BPM)

    # ì¹´ìš´íŠ¸ì¸(ë§ˆë””) -> ms ê³„ì‚°
    countin_ms, countin_beats = bars_to_sleep_ms(
        COUNTIN_BARS, METRONOME_BPM, SAMPLE_RATE, BEATS_PER_BAR
    )

    print(f"{'='*70}")
    print(f"ğŸ¸ 4ë¹„íŠ¸ ë©”íŠ¸ë¡œë†ˆ í¬ë¡œë§¤í‹± ë¶„ì„ê¸° ({METRONOME_BPM} BPM)")
    print(f"{'='*70}")
    print(f"â±ï¸  ì¹´ìš´íŠ¸ì¸: {COUNTIN_BARS} bar ({countin_beats} beats)")
    print(f"ğŸ“Š ë…¹ìŒ ì‹œê°„: {RECORD_DURATION}ì´ˆ")
    print(f"ğŸµ ì„¤ì •: ëª¨ë“  4ë°•ì ì†Œë¦¬ ì¶œë ¥ (8ë¶„ìŒí‘œ ì†Œë¦¬ ì œê±°)")
    print(f"{'='*70}\n")

    try:
        info = sd.query_devices(ASIO_DEVICE_ID)
        channels = min(info["max_input_channels"], info["max_output_channels"])

        with sd.Stream(
            device=ASIO_DEVICE_ID,
            samplerate=SAMPLE_RATE,
            blocksize=BLOCK_SIZE,
            dtype="float32",
            channels=channels,
            callback=audio_callback,
        ):
            metronome_active = True

            print(f"ğŸ¼ ì¹´ìš´íŠ¸ì¸ ì‹œì‘! ({COUNTIN_BARS} bar) 4ë¹„íŠ¸ ì†Œë¦¬ì— ë§ì¶”ì–´ ì¤€ë¹„í•˜ì„¸ìš”...")
            sd.sleep(countin_ms)

            # ë…¹ìŒ ì‹œì‘ì„ ê°•ë°• ê¸°ì¤€ìœ¼ë¡œ ëª…í™•íˆ ì •ë ¬(ê¶Œì¥)
            current_beat = 0
            sample_counter = 0

            print("\nğŸš€ ë…¹ìŒ ì‹œì‘! í¬ë¡œë§¤í‹± ì—°ì£¼ Go!\n")
            is_recording = True

            for i in range(RECORD_DURATION, 0, -1):
                bar = "â–ˆ" * (RECORD_DURATION - i + 1) + "â–‘" * (i - 1)
                print(f"  â±ï¸  [{bar}] {i:2d}ì´ˆ ë‚¨ìŒ", end="\r")
                sd.sleep(1000)

            metronome_active = False
            is_recording = False

        print("\n\nâœ“ ë…¹ìŒ ì™„ë£Œ! ë¶„ì„ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")

        audio_array = np.array(recorded_data, dtype=np.float32)
        fig = create_waveform_with_metronome(audio_array, METRONOME_BPM, SAMPLE_RATE)

        filename = save_analysis_image(fig)
        img_base64 = fig_to_base64(fig)

        print("\n" + "=" * 70)
        print("ğŸ“Š ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ!")
        print("=" * 70)
        print(f"âœ… ì´ë¯¸ì§€ íŒŒì¼: {filename}")
        print("=" * 70)

        plt.show()

        return filename, img_base64

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    record_and_analyze()
